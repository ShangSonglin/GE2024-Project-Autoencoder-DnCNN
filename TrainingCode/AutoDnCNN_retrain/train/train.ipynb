{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965d0d84-bad6-494f-97cd-4c4b45c77a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T16:06:01.589826Z",
     "iopub.status.busy": "2024-12-10T16:06:01.589482Z",
     "iopub.status.idle": "2024-12-11T01:19:03.485671Z",
     "shell.execute_reply": "2024-12-11T01:19:03.484278Z",
     "shell.execute_reply.started": "2024-12-10T16:06:01.589805Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 60000\n",
      "Batch size: 64\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "init weight\n",
      "Autoencoder Model:\n",
      "ConvAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "DnCNN Model:\n",
      "DnCNN(\n",
      "  (dncnn): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (18): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (21): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (24): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (27): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (30): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (33): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (34): ReLU(inplace=True)\n",
      "    (35): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (36): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (37): ReLU(inplace=True)\n",
      "    (38): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (39): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (40): ReLU(inplace=True)\n",
      "    (41): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (42): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (43): ReLU(inplace=True)\n",
      "    (44): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (45): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (46): ReLU(inplace=True)\n",
      "    (47): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (48): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (49): ReLU(inplace=True)\n",
      "    (50): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (51): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (52): ReLU(inplace=True)\n",
      "    (53): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (54): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (55): ReLU(inplace=True)\n",
      "    (56): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (57): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (58): ReLU(inplace=True)\n",
      "    (59): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (60): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (61): ReLU(inplace=True)\n",
      "    (62): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (63): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (64): ReLU(inplace=True)\n",
      "    (65): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (66): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (67): ReLU(inplace=True)\n",
      "    (68): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (69): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (70): ReLU(inplace=True)\n",
      "    (71): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (72): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (73): ReLU(inplace=True)\n",
      "    (74): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (75): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (76): ReLU(inplace=True)\n",
      "    (77): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (78): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (79): ReLU(inplace=True)\n",
      "    (80): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (81): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (82): ReLU(inplace=True)\n",
      "    (83): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (84): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (85): ReLU(inplace=True)\n",
      "    (86): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (87): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (88): ReLU(inplace=True)\n",
      "    (89): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (90): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (91): ReLU(inplace=True)\n",
      "    (92): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (93): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (94): ReLU(inplace=True)\n",
      "    (95): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (96): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (97): ReLU(inplace=True)\n",
      "    (98): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (99): BatchNorm2d(128, eps=0.0001, momentum=0.95, affine=True, track_running_stats=True)\n",
      "    (100): ReLU(inplace=True)\n",
      "    (101): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "Epoch 1/10\n",
      "Batch 1/938\n",
      "Autoencoder Loss: 1.9921\n",
      "DnCNN Loss: 1.1024\n",
      "Batch 101/938\n",
      "Autoencoder Loss: 0.8807\n",
      "DnCNN Loss: 0.0183\n",
      "Batch 201/938\n",
      "Autoencoder Loss: 0.8803\n",
      "DnCNN Loss: 0.0168\n",
      "Batch 301/938\n",
      "Autoencoder Loss: 0.8789\n",
      "DnCNN Loss: 0.0160\n",
      "Batch 401/938\n",
      "Autoencoder Loss: 0.8777\n",
      "DnCNN Loss: 0.0153\n",
      "Batch 501/938\n",
      "Autoencoder Loss: 0.8791\n",
      "DnCNN Loss: 0.0148\n",
      "Batch 601/938\n",
      "Autoencoder Loss: 0.8836\n",
      "DnCNN Loss: 0.0139\n",
      "Batch 701/938\n",
      "Autoencoder Loss: 0.8785\n",
      "DnCNN Loss: 0.0146\n",
      "Batch 801/938\n",
      "Autoencoder Loss: 0.8817\n",
      "DnCNN Loss: 0.0140\n",
      "Batch 901/938\n",
      "Autoencoder Loss: 0.8805\n",
      "DnCNN Loss: 0.0154\n",
      "Epoch [1/10], Autoencoder Loss: 0.8782, DnCNN Loss: 0.0144\n",
      "Epoch 2/10\n",
      "Batch 1/938\n",
      "Autoencoder Loss: 0.8812\n",
      "DnCNN Loss: 0.0140\n",
      "Batch 101/938\n",
      "Autoencoder Loss: 0.8865\n",
      "DnCNN Loss: 0.0128\n",
      "Batch 201/938\n",
      "Autoencoder Loss: 0.8744\n",
      "DnCNN Loss: 0.0144\n",
      "Batch 301/938\n",
      "Autoencoder Loss: 0.8799\n",
      "DnCNN Loss: 0.0132\n",
      "Batch 401/938\n",
      "Autoencoder Loss: 0.8740\n",
      "DnCNN Loss: 0.0123\n",
      "Batch 501/938\n",
      "Autoencoder Loss: 0.8852\n",
      "DnCNN Loss: 0.0124\n",
      "Batch 601/938\n",
      "Autoencoder Loss: 0.8811\n",
      "DnCNN Loss: 0.0104\n",
      "Batch 701/938\n",
      "Autoencoder Loss: 0.8761\n",
      "DnCNN Loss: 0.0106\n",
      "Batch 801/938\n",
      "Autoencoder Loss: 0.8774\n",
      "DnCNN Loss: 0.0100\n",
      "Batch 901/938\n",
      "Autoencoder Loss: 0.8773\n",
      "DnCNN Loss: 0.0109\n",
      "Epoch [2/10], Autoencoder Loss: 0.8752, DnCNN Loss: 0.0120\n",
      "Epoch 3/10\n",
      "Batch 1/938\n",
      "Autoencoder Loss: 0.8739\n",
      "DnCNN Loss: 0.0106\n",
      "Batch 101/938\n",
      "Autoencoder Loss: 0.8748\n",
      "DnCNN Loss: 0.0096\n",
      "Batch 201/938\n",
      "Autoencoder Loss: 0.8757\n",
      "DnCNN Loss: 0.0104\n",
      "Batch 301/938\n",
      "Autoencoder Loss: 0.8812\n",
      "DnCNN Loss: 0.0096\n",
      "Batch 401/938\n",
      "Autoencoder Loss: 0.8858\n",
      "DnCNN Loss: 0.0086\n",
      "Batch 501/938\n",
      "Autoencoder Loss: 0.8795\n",
      "DnCNN Loss: 0.0097\n",
      "Batch 601/938\n",
      "Autoencoder Loss: 0.8770\n",
      "DnCNN Loss: 0.0106\n",
      "Batch 701/938\n",
      "Autoencoder Loss: 0.8771\n",
      "DnCNN Loss: 0.0103\n",
      "Batch 801/938\n",
      "Autoencoder Loss: 0.8797\n",
      "DnCNN Loss: 0.0132\n",
      "Batch 901/938\n",
      "Autoencoder Loss: 0.8787\n",
      "DnCNN Loss: 0.0096\n",
      "Epoch [3/10], Autoencoder Loss: 0.8707, DnCNN Loss: 0.0103\n",
      "Epoch 4/10\n",
      "Batch 1/938\n",
      "Autoencoder Loss: 0.8787\n",
      "DnCNN Loss: 0.0087\n",
      "Batch 101/938\n",
      "Autoencoder Loss: 0.8823\n",
      "DnCNN Loss: 0.0081\n",
      "Batch 201/938\n",
      "Autoencoder Loss: 0.8746\n",
      "DnCNN Loss: 0.0115\n",
      "Batch 301/938\n",
      "Autoencoder Loss: 0.8874\n",
      "DnCNN Loss: 0.0099\n",
      "Batch 401/938\n",
      "Autoencoder Loss: 0.8812\n",
      "DnCNN Loss: 0.0087\n",
      "Batch 501/938\n",
      "Autoencoder Loss: 0.8830\n",
      "DnCNN Loss: 0.0084\n",
      "Batch 601/938\n",
      "Autoencoder Loss: 0.8767\n",
      "DnCNN Loss: 0.0096\n",
      "Batch 701/938\n",
      "Autoencoder Loss: 0.8835\n",
      "DnCNN Loss: 0.0095\n",
      "Batch 801/938\n",
      "Autoencoder Loss: 0.8781\n",
      "DnCNN Loss: 0.0090\n",
      "Batch 901/938\n",
      "Autoencoder Loss: 0.8793\n",
      "DnCNN Loss: 0.0081\n",
      "Epoch [4/10], Autoencoder Loss: 0.8816, DnCNN Loss: 0.0084\n",
      "Epoch 5/10\n",
      "Batch 1/938\n",
      "Autoencoder Loss: 0.8788\n",
      "DnCNN Loss: 0.0088\n",
      "Batch 101/938\n",
      "Autoencoder Loss: 0.8854\n",
      "DnCNN Loss: 0.0089\n",
      "Batch 201/938\n",
      "Autoencoder Loss: 0.8816\n",
      "DnCNN Loss: 0.0086\n",
      "Batch 301/938\n",
      "Autoencoder Loss: 0.8748\n",
      "DnCNN Loss: 0.0091\n",
      "Batch 401/938\n",
      "Autoencoder Loss: 0.8814\n",
      "DnCNN Loss: 0.0085\n",
      "Batch 501/938\n",
      "Autoencoder Loss: 0.8784\n",
      "DnCNN Loss: 0.0087\n",
      "Batch 601/938\n",
      "Autoencoder Loss: 0.8789\n",
      "DnCNN Loss: 0.0095\n",
      "Batch 701/938\n",
      "Autoencoder Loss: 0.8789\n",
      "DnCNN Loss: 0.0087\n",
      "Batch 801/938\n",
      "Autoencoder Loss: 0.8830\n",
      "DnCNN Loss: 0.0089\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 141\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型权重已保存为 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoencoder_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 和 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdncnn_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mtrain_and_test_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 116\u001b[0m, in \u001b[0;36mtrain_and_test_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m autoencoder_outputs \u001b[38;5;241m=\u001b[39m autoencoder_model(noisy_images)\n\u001b[1;32m    115\u001b[0m loss_autoencoder \u001b[38;5;241m=\u001b[39m criterion(autoencoder_outputs, clean_images)\n\u001b[0;32m--> 116\u001b[0m \u001b[43mloss_autoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m optimizer_autoencoder\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# DnCNN去噪\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.init as init\n",
    "\n",
    "# 定义ConvAutoencoder和DnCNN模型\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        # 编码器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 解码器\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, depth=17, n_channels=64, image_channels=1, use_bnorm=True, kernel_size=3):\n",
    "        super(DnCNN, self).__init__()\n",
    "        kernel_size = 3\n",
    "        padding = 1\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding,\n",
    "                      bias=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(depth - 2):\n",
    "            layers.append(\n",
    "                nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size, padding=padding,\n",
    "                          bias=False))\n",
    "            layers.append(nn.BatchNorm2d(n_channels, eps=0.0001, momentum=0.95))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels=n_channels, out_channels=image_channels, kernel_size=kernel_size, padding=padding,\n",
    "                      bias=False))\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        out = self.dncnn(x)\n",
    "        return y - out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.orthogonal_(m.weight)\n",
    "                print('init weight')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "def train_and_test_models():\n",
    "    num_epochs = 10  # 增加训练的epoch数量\n",
    "    batch_size = 64  # 调整批次大小\n",
    "    learning_rate = 0.001  # 调整学习率\n",
    "\n",
    "    # 数据预处理和加载数据集\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.RandomRotation(10),  # 添加随机旋转的数据增强\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "    # 初始化模型\n",
    "    autoencoder_model = ConvAutoencoder()\n",
    "    dncnn_model = DnCNN(depth=35, n_channels=128)  # 调整DnCNN的深度和通道数\n",
    "\n",
    "    print(\"Autoencoder Model:\")\n",
    "    print(autoencoder_model)\n",
    "    print(\"DnCNN Model:\")\n",
    "    print(dncnn_model)\n",
    "\n",
    "    # 初始化优化器\n",
    "    optimizer_autoencoder = optim.Adam(autoencoder_model.parameters(), lr=learning_rate)\n",
    "    optimizer_dncnn = optim.Adam(dncnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 损失函数\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            clean_images, _ = batch\n",
    "            noisy_images = clean_images + torch.randn_like(clean_images) * 0.3\n",
    "            noisy_images = torch.clamp(noisy_images, 0., 1.)\n",
    "\n",
    "            # 自动编码器去噪\n",
    "            optimizer_autoencoder.zero_grad()\n",
    "            autoencoder_outputs = autoencoder_model(noisy_images)\n",
    "            loss_autoencoder = criterion(autoencoder_outputs, clean_images)\n",
    "            loss_autoencoder.backward()\n",
    "            optimizer_autoencoder.step()\n",
    "\n",
    "            # DnCNN去噪\n",
    "            optimizer_dncnn.zero_grad()\n",
    "            dncnn_outputs = dncnn_model(noisy_images)\n",
    "            loss_dncnn = criterion(dncnn_outputs, clean_images)\n",
    "            loss_dncnn.backward()\n",
    "            optimizer_dncnn.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:  # 每100个批次打印一次\n",
    "                print(f\"Batch {batch_idx + 1}/{len(dataloader)}\")\n",
    "                print(f\"Autoencoder Loss: {loss_autoencoder.item():.4f}\")\n",
    "                print(f\"DnCNN Loss: {loss_dncnn.item():.4f}\")\n",
    "\n",
    "        # 打印每个epoch的损失\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], Autoencoder Loss: {loss_autoencoder.item():.4f}, DnCNN Loss: {loss_dncnn.item():.4f}\")\n",
    "\n",
    "    # 保存权重\n",
    "    torch.save(autoencoder_model.state_dict(), 'autoencoder_weights.pth')\n",
    "    torch.save(dncnn_model.state_dict(), 'dncnn_weights.pth')\n",
    "    print(\"模型权重已保存为 'autoencoder_weights.pth' 和 'dncnn_weights.pth'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_test_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
